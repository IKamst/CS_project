{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT.ipynb","provenance":[],"mount_file_id":"1dW342ah_X_M_9tUEbxCTSr03edAg0wAW","authorship_tag":"ABX9TyN0yYDwwJIYNpVkrGBm8aue"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yOov23Q6Qfku","executionInfo":{"status":"ok","timestamp":1642516692662,"user_tz":-60,"elapsed":3900,"user":{"displayName":"Rik Zijlema","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16715734862350772008"}},"outputId":"b83fc1bc-e06e-4690-83b3-3456c85205a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.62.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.20.37)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.10.0+cu111)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.10.0.2)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n","Requirement already satisfied: botocore<1.24.0,>=1.23.37 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.23.37)\n","Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.37->boto3->pytorch_pretrained_bert) (1.25.11)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.37->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.37->boto3->pytorch_pretrained_bert) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n"]}],"source":["!pip install pytorch_pretrained_bert\n","import os\n","from tqdm import tqdm_notebook as tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils import data\n","import torch.optim as optim\n","from pytorch_pretrained_bert import BertTokenizer\n","from sklearn.metrics import precision_score, recall_score, f1_score"]},{"cell_type":"code","source":["from google.colab import drive \n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/CS_project/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PhNniv0ATMMd","executionInfo":{"status":"ok","timestamp":1642515517492,"user_tz":-60,"elapsed":1584,"user":{"displayName":"Rik Zijlema","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16715734862350772008"}},"outputId":"f0337102-58c2-47f3-9c95-c01703713971"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/CS_project\n"]}]},{"cell_type":"code","source":["from read_sentences import read_file\n","train_sent, train_tagged_sent = read_file(\"gold_data/train.conll\")\n","eval_sent, eval_tagged_sent = read_file(\"gold_data/dev.conll\")\n","\n","tags = list(set(word_tag[1] for sent in train_tagged_sent for word_tag in sent))\n","\",\".join(tags)\n","tags = [\"<pad>\"] + tags\n","\n","tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n","idx2tag = {idx:tag for idx, tag in enumerate(tags)}"],"metadata":{"id":"Gp1einV-RiZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"gugpr6kMj05O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"],"metadata":{"id":"qtdUvQ8Ij17A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SMTDataset(data.Dataset):\n","    def __init__(self, tagged_sents):\n","        sents, tags_li = [], []\n","        for sent in tagged_sents:\n","            words = [word_tag[0] for word_tag in sent]\n","            tags = [word_tag[1] for word_tag in sent]\n","            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n","            tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])\n","        self.sents, self.tags_li = sents, tags_li\n","\n","    def __len__(self):\n","        return len(self.sents)\n","\n","    def __getitem__(self, idx):\n","        words, tags = self.sents[idx], self.tags_li[idx]\n","\n","        x, y = [], []\n","        is_heads = []\n","        for w, t in zip(words, tags):\n","            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n","            xx = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            is_head = [1] + [0]*(len(tokens) - 1)\n","\n","            t = [t] + [\"<pad>\"] * (len(tokens) - 1)\n","            yy = [tag2idx[each] for each in t]\n","\n","            x.extend(xx)\n","            is_heads.extend(is_head)\n","            y.extend(yy)\n","\n","        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n","\n","        seqlen = len(y)\n","\n","        words = \" \".join(words)\n","        tags = \" \".join(tags)\n","        return words, x, is_heads, tags, y, seqlen"],"metadata":{"id":"3TnrYIf8j6JA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad(batch):\n","    f = lambda x: [sample[x] for sample in batch]\n","    words = f(0)\n","    is_heads = f(2)\n","    tags = f(3)\n","    seqlens = f(-1)\n","    maxlen = np.array(seqlens).max()\n","\n","    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch]\n","    x = f(1, maxlen)\n","    y = f(-2, maxlen)\n","\n","\n","    f = torch.LongTensor\n","\n","    return words, f(x), is_heads, tags, f(y), seqlens"],"metadata":{"id":"z4qHj69Vj-KJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pytorch_pretrained_bert import BertModel"],"metadata":{"id":"Y0xC1MULkApX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self, vocab_size=None):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-cased')\n","\n","        self.fc = nn.Linear(768, vocab_size)\n","        self.device = device\n","\n","    def forward(self, x, y):\n","\n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        if self.training:\n","            self.bert.train()\n","            encoded_layers, _ = self.bert(x)\n","            enc = encoded_layers[-1]\n","        else:\n","            self.bert.eval()\n","            with torch.no_grad():\n","                encoded_layers, _ = self.bert(x)\n","                enc = encoded_layers[-1]\n","        \n","        logits = self.fc(enc)\n","        y_hat = logits.argmax(-1)\n","        return logits, y, y_hat"],"metadata":{"id":"04iTbRM_kEKx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion):\n","    model.train()\n","    for i, batch in enumerate(iterator):\n","        words, x, is_heads, tags, y, seqlens = batch\n","        _y = y\n","        optimizer.zero_grad()\n","        logits, y, _ = model(x, y)\n","\n","        logits = logits.view(-1, logits.shape[-1])\n","        y = y.view(-1)\n","\n","        loss = criterion(logits, y)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        if i%10==0:\n","            print(\"step: {}, loss: {}\".format(i, loss.item()))"],"metadata":{"id":"xEgZi5aDkG7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval(model, iterator):\n","    model.eval()\n","\n","    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","            words, x, is_heads, tags, y, seqlens = batch\n","\n","            _, _, y_hat = model(x, y)\n","\n","            Words.extend(words)\n","            Is_heads.extend(is_heads)\n","            Tags.extend(tags)\n","            Y.extend(y.numpy().tolist())\n","            Y_hat.extend(y_hat.cpu().numpy().tolist())\n","\n","    # get results and save\n","    with open(\"result\", 'w') as fout:\n","        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n","            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n","            preds = [idx2tag[hat] for hat in y_hat]\n","            assert len(preds)==len(words.split())==len(tags.split())\n","            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n","                fout.write(\"{} {} {}\\n\".format(w, t, p))\n","            fout.write(\"\\n\")\n","            \n","    # calculate metrics\n","    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n","\n","    print(\"Precision: \", precision_score(y_true, y_pred, average=\"macro\"))\n","    print(\"Recall: \", recall_score(y_true, y_pred, average=\"macro\"))\n","    print(\"F-score: \", f1_score(y_true,y_pred, average=\"macro\"))"],"metadata":{"id":"73ssnRQckLbz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net(vocab_size=len(tag2idx))\n","model.to(device)\n","model = nn.DataParallel(model)"],"metadata":{"id":"IXTJjT9VkOYr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = SMTDataset(train_tagged_sent)\n","eval_dataset = SMTDataset(eval_tagged_sent)\n","\n","train_iter = data.DataLoader(dataset=train_dataset,\n","                             batch_size=8,\n","                             shuffle=True,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","test_iter = data.DataLoader(dataset=eval_dataset,\n","                             batch_size=8,\n","                             shuffle=False,\n","                             num_workers=1,\n","                             collate_fn=pad)\n","\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0)"],"metadata":{"id":"2jluCSS-kXc6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(model, train_iter, optimizer, criterion)\n","eval(model, test_iter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49Mjtj7pkeIA","executionInfo":{"status":"ok","timestamp":1642517380406,"user_tz":-60,"elapsed":191973,"user":{"displayName":"Rik Zijlema","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16715734862350772008"}},"outputId":"aeea61cd-10c2-4684-deef-fabe732763e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["step: 0, loss: 4.3982253074646\n","step: 10, loss: 2.2886760234832764\n","step: 20, loss: 1.5200132131576538\n","step: 30, loss: 0.7758545279502869\n","step: 40, loss: 0.7722137570381165\n","step: 50, loss: 0.4944884777069092\n","step: 60, loss: 0.18191733956336975\n","step: 70, loss: 0.7864298820495605\n","step: 80, loss: 0.2335485816001892\n","step: 90, loss: 0.35760486125946045\n","step: 100, loss: 0.24922101199626923\n","step: 110, loss: 0.19836334884166718\n","step: 120, loss: 0.4741491973400116\n","step: 130, loss: 0.16762320697307587\n","step: 140, loss: 0.837109386920929\n","step: 150, loss: 0.3313911259174347\n","step: 160, loss: 0.0888025090098381\n","step: 170, loss: 0.05739789083600044\n","step: 180, loss: 0.3000776171684265\n","step: 190, loss: 0.21507593989372253\n","step: 200, loss: 0.26703014969825745\n","step: 210, loss: 0.18615789711475372\n","step: 220, loss: 0.046687956899404526\n","step: 230, loss: 0.19792869687080383\n","step: 240, loss: 0.05871516093611717\n","step: 250, loss: 0.02592264860868454\n","step: 260, loss: 0.1394789218902588\n","step: 270, loss: 0.18709470331668854\n","step: 280, loss: 0.21315446496009827\n","step: 290, loss: 0.44972437620162964\n","step: 300, loss: 0.228188619017601\n","step: 310, loss: 0.0916140079498291\n","step: 320, loss: 0.21840234100818634\n","step: 330, loss: 0.023175103589892387\n","step: 340, loss: 0.043312203139066696\n","step: 350, loss: 0.0941862016916275\n","step: 360, loss: 0.07734155654907227\n","step: 370, loss: 0.06175459176301956\n","step: 380, loss: 0.027343405410647392\n","step: 390, loss: 0.12517887353897095\n","step: 400, loss: 0.08873870968818665\n","step: 410, loss: 0.7763661742210388\n","step: 420, loss: 0.04775538295507431\n","step: 430, loss: 0.102711521089077\n","step: 440, loss: 0.18466565012931824\n","step: 450, loss: 0.03812650591135025\n","step: 460, loss: 0.07216136157512665\n","step: 470, loss: 0.21312583982944489\n","step: 480, loss: 0.17970898747444153\n","step: 490, loss: 0.07827651500701904\n","step: 500, loss: 0.304115891456604\n","step: 510, loss: 0.11361072957515717\n","step: 520, loss: 0.14719538390636444\n","step: 530, loss: 0.46232590079307556\n","step: 540, loss: 0.14103896915912628\n","step: 550, loss: 0.17712773382663727\n","step: 560, loss: 0.2078118920326233\n","step: 570, loss: 0.09560412913560867\n","step: 580, loss: 0.2655578553676605\n","step: 590, loss: 0.11741525679826736\n","step: 600, loss: 0.05036737397313118\n","step: 610, loss: 0.14564719796180725\n","step: 620, loss: 0.07003773748874664\n","step: 630, loss: 0.09602237492799759\n","step: 640, loss: 0.049650195986032486\n","step: 650, loss: 0.32210540771484375\n","step: 660, loss: 0.052982572466135025\n","step: 670, loss: 0.139952152967453\n","step: 680, loss: 0.06403848528862\n","step: 690, loss: 0.06378962844610214\n","step: 700, loss: 0.04652823880314827\n","step: 710, loss: 0.04941640421748161\n","step: 720, loss: 0.1825922280550003\n","step: 730, loss: 0.04621152952313423\n","step: 740, loss: 0.07819488644599915\n","step: 750, loss: 0.09517573565244675\n","step: 760, loss: 0.09710922837257385\n","step: 770, loss: 0.049681708216667175\n","step: 780, loss: 0.035528041422367096\n","step: 790, loss: 0.022614261135458946\n","step: 800, loss: 0.026220204308629036\n","step: 810, loss: 0.23545509576797485\n","step: 820, loss: 0.039367783814668655\n","step: 830, loss: 0.01182518620043993\n","step: 840, loss: 0.09533414244651794\n","step: 850, loss: 0.053740933537483215\n","step: 860, loss: 0.13819938898086548\n","step: 870, loss: 0.13096193969249725\n","step: 880, loss: 0.030509060248732567\n","step: 890, loss: 0.09894037246704102\n","step: 900, loss: 0.12422254681587219\n","step: 910, loss: 0.06616732478141785\n","step: 920, loss: 0.24388277530670166\n","step: 930, loss: 0.0976748839020729\n","step: 940, loss: 0.14067016541957855\n","step: 950, loss: 0.09209402650594711\n","step: 960, loss: 0.042718712240457535\n","Precision:  0.9056093917929965\n","Recall:  0.8479240557328379\n","F-score:  0.8578110146165536\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["open('result', 'r').read().splitlines()[:250]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Knk74t0AlSxq","executionInfo":{"status":"ok","timestamp":1642517401803,"user_tz":-60,"elapsed":213,"user":{"displayName":"Rik Zijlema","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16715734862350772008"}},"outputId":"bfa75692-f858-477e-86e0-5be4e02eae45"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['A DIS DIS',\n"," 'brown COL COL',\n"," 'dog CON CON',\n"," 'and GRP GRP',\n"," 'a DIS DIS',\n"," 'grey COL COL',\n"," 'dog CON CON',\n"," 'are NOW NOW',\n"," 'fighting EXG EXG',\n"," 'in REL REL',\n"," 'the DEF DEF',\n"," 'snow CON CON',\n"," '',\n"," 'A DIS DIS',\n"," 'woman CON CON',\n"," 'in REL REL',\n"," 'a DIS DIS',\n"," 'white COL COL',\n"," 'dress CON CON',\n"," 'and GRP GRP',\n"," 'a DIS DIS',\n"," 'woman CON CON',\n"," 'in REL REL',\n"," 'a DIS DIS',\n"," 'blue COL COL',\n"," 'dress CON CON',\n"," 'are NOW NOW',\n"," 'standing EXG EXG',\n"," 'on REL REL',\n"," 'a DIS DIS',\n"," 'stage CON CON',\n"," '',\n"," 'There NIL NIL',\n"," 'is NOW NOW',\n"," 'no NOT NOT',\n"," 'man CON CON',\n"," 'playing EXG EXG',\n"," 'two QUC QUC',\n"," 'keyboards CON CON',\n"," '',\n"," 'There NIL NIL',\n"," 'is NOW NOW',\n"," 'no NOT NOT',\n"," 'person CON CON',\n"," 'cutting EXG EXG',\n"," 'some DIS DIS',\n"," 'ginger CON CON',\n"," '',\n"," 'You PRO PRO',\n"," 'are NOW NOW',\n"," 'screwed EXS IST',\n"," '. NIL NIL',\n"," '',\n"," 'This PRX PRX',\n"," 'is ENS ENS',\n"," 'a DIS DIS',\n"," 'man-made IST IST',\n"," 'language CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'which QUE QUE',\n"," 'singer ROL ROL',\n"," 'do NOW NOW',\n"," 'you PRO PRO',\n"," 'like EXS EXS',\n"," '? QUE QUE',\n"," '',\n"," 'Kraft ORG ORG',\n"," 'sold EPS EPS',\n"," 'Celestial~Seasonings ORG ORG',\n"," '. NIL NIL',\n"," '',\n"," 'Anna~Politkovskaya PER PER',\n"," 'was PST PST',\n"," 'murdered EXS EXS',\n"," '. NIL NIL',\n"," '',\n"," 'Alfred~Nobel PER PER',\n"," 'is ENS ENS',\n"," 'the DEF DEF',\n"," 'inventor ROL ROL',\n"," 'of REL REL',\n"," 'dynamite CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'The DEF DEF',\n"," 'yakuza CON CON',\n"," 'are ENS ENS',\n"," 'the DEF DEF',\n"," 'Japanese GPO GPO',\n"," 'mafia CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'Russia GPE GPE',\n"," 'fears ENS ENS',\n"," 'the DEF DEF',\n"," 'system CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'Bountiful ART EXG',\n"," 'reached EPS EPS',\n"," 'San~Francisco GPE GPE',\n"," 'on REL REL',\n"," '1 DOM DOM',\n"," 'November MOY MOY',\n"," '1945 YOC YOC',\n"," '. NIL NIL',\n"," '',\n"," 'Pierce PER PER',\n"," 'lives ENS ENS',\n"," 'near APX APX',\n"," 'Rossville~Blvd GEO GPE',\n"," '. NIL NIL',\n"," '',\n"," 'Yunus PER PER',\n"," 'founded EPS EPS',\n"," 'the DEF DEF',\n"," 'Grameen~Bank ORG ART',\n"," '30 QUC QUC',\n"," 'years UOM UOM',\n"," 'ago PST PST',\n"," '. NIL NIL',\n"," '',\n"," 'Maria PER PER',\n"," 'has ENS ENS',\n"," 'long DEG DEG',\n"," 'hair CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'How EMP EMP',\n"," 'slow IST IST',\n"," 'you PRO PRO',\n"," 'are NOW NOW',\n"," '! EMP EMP',\n"," '',\n"," 'The DEF DEF',\n"," 'astronauts ROL ROL',\n"," 'went EPS EPS',\n"," 'up REL TOP',\n"," 'to REL REL',\n"," 'the DEF DEF',\n"," 'moon CON CON',\n"," 'in REL REL',\n"," 'a DIS DIS',\n"," 'rocket CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'We PRO PRO',\n"," 'stood EPS EPS',\n"," 'at REL REL',\n"," 'the DEF DEF',\n"," 'door CON CON',\n"," 'and AND COO',\n"," 'waited EPS EPS',\n"," '. NIL NIL',\n"," '',\n"," 'Bob PER PER',\n"," 'is NOW NOW',\n"," 'popular IST IST',\n"," 'at REL REL',\n"," 'school CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'Bill PER PER',\n"," 'was PST PST',\n"," 'killed EXS EXS',\n"," 'by REL REL',\n"," 'an DIS DIS',\n"," 'intruder ROL ROL',\n"," '. NIL NIL',\n"," '',\n"," 'It NIL NIL',\n"," 'took EPS EPS',\n"," 'me PRO PRO',\n"," 'two QUC QUC',\n"," 'hours UOM UOM',\n"," 'to NIL NIL',\n"," 'memorize EXS EXS',\n"," 'this PRX PRX',\n"," 'sentence CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'Whose HAS HAS',\n"," 'is ENS ENS',\n"," 'this PRX PRX',\n"," 'camera CON CON',\n"," '? QUE QUE',\n"," '',\n"," 'I PRO PRO',\n"," \"'m NOW NOW\",\n"," 'calling EXG EXG',\n"," 'because SUB SUB',\n"," 'I PRO PRO',\n"," \"'ve NOW NOW\",\n"," 'lost EXT EXT',\n"," 'my HAS HAS',\n"," 'credit~card CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'Let FUT FUT',\n"," \"'s PRO CON\",\n"," 'have EXS EXS',\n"," 'sushi CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'The DEF DEF',\n"," 'dog CON CON',\n"," 'attacked EPS EPS',\n"," 'the DEF DEF',\n"," 'little IST IST',\n"," 'boy CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'Think EXS EXS',\n"," 'about REL REL',\n"," 'it PRO PRO',\n"," '. NIL NIL',\n"," '',\n"," 'I PRO PRO',\n"," 'like ENS ENS',\n"," 'the DEF DEF',\n"," 'actor ROL ROL',\n"," '. NIL NIL',\n"," '',\n"," 'I PRO PRO',\n"," 'gave EPS EPS',\n"," 'it PRO PRO',\n"," 'to REL REL',\n"," 'my HAS HAS',\n"," 'mommy ROL ROL',\n"," '. NIL NIL',\n"," '',\n"," 'I PRO PRO',\n"," 'bought EPS EPS',\n"," 'a DIS DIS',\n"," 'few QUV QUV',\n"," 'eggs CON CON',\n"," 'and GRP GRP',\n"," 'a DIS DIS',\n"," 'little QUV IST',\n"," 'milk CON CON',\n"," '. NIL NIL',\n"," '',\n"," 'My HAS HAS',\n"," 'uncle ROL ROL',\n"," 'bought EPS EPS',\n"," 'me PRO PRO',\n"," 'this PRX PRX',\n"," 'book CON CON',\n"," '. NIL NIL']"]},"metadata":{},"execution_count":38}]}]}